CIB: Purpose
==========================
The CIB is described quit well in section 5 of lmb's crm.txt (checked into CVS in the crm directory) so I wont repeat everything here.  Suffice to say that it stores the configuration and runtime data required for cluster-wide resource managment.

CIB: External Representation
===========================
I believe there have been a number of discussions on this and I'm going to take it as given that we are all (if not grudgingly) agreed that the CIB will be passed around as XML.

CIB: Internal Representation
===========================
Internally, the CIB will be stored and manipulated as a libxml2 (as used by stonith code, so this is not a new dependancy) XML document.  This may not be the most efficient data representation but the trade off is that we dont spend half our time encoding/decoding everytime messages are sent/retrieved.  It also means that fields can be added and deprecated more easily if required (ie. no encode/decode methods to rewrite) and, in theory, should allow for better compatibility between CIB versions.

CIB: Information Structure
===========================
The CIB is divided up into 4 main sections: nodes, resource, constraints and status.  It contains a version string, a flag to indicate if this was read from a file or generated (possibly should be replaced with a generation number) and a timestamp.

All information carrying elements carry an "id" field which is an alpha-numeric string which must also be unique for that element type, a description to aid administration and a timestamp.  The timestamps are required for the DC (Designated Controller) to determine the latest cluster-wide configuration.  

Resources carry a "max_instances", an "op_timeout" and a "priority" field.  The op_timeout is intended for use by the LRM and CRM in order to determine when to start worrying about the length of time (in seconds) a start, stop or restart is taking.  The priority is used by the Policy Engine in order to determine the order in which to process dependancies and more drastically to know which resources to leave out if all dependancies can not be satisfied.  The max_instances field is intended to address the replicated resource issue.  When a resource is started, it is told which instance number it has and it is then up to it to decide if it should be a master, a slave or some other variation, as the resource itself is the best equiped to do so (Ie. the resource knows if there can be 1, 2 or 20 masters).

Constraints have their own section as many (such as startAfter, startBefore) will be relevant to more than one resource, making the resource the wrong place to store them.  Constraints may be put in place by the CRM automatically, for example after a failed startup, and these will need to be cleared at some point.  The clear_on attribute is intended to determin when this point should be.

Resource state information is also separated out as it this section is "runtime" data and is "compiled" by the DC from information sent by all nodes in the cluster.  An argument could also be made that the same could be done for node_status and health, however the primary source of this data is the CCM (ie. not compiled) and information about "bad" nodes should persist even after all good nodes have exited.  I will leave this a matter for debate if people feel strongly about it.


In places, a "source" attribute has been added so that the CRM is able to know where this information came from.  This is helpful during the merging process (performed by a new DC and perhaps periodically) as it allows the CRM to allow nodes to be authoritive about themselves if appropriate (ie. which resources it is running, but perhaps not always about its own health).

To avoid duplication of data, state entries only carry references to nodes and resources.  Likewise resources and constraints only contain references to the nodes that they can (or explicitly cannot via a negative weight) run on.

CIB: Messaging
===========================
The rationale behind the "cib_operation" and "cib_section" elements of the command is to allow the DC the ability, if desired, to optimise the CIB's performance querying for and updating only certain sections of the CIB.  In the initial prototype, this will most likely not be used.

CIB: Schema/DTD
===========================

<!--
Internally to the CIB and when written to disk, "cib" is used as the root element.  However when sent as a message to another node or sub-system (the schema describing this will be available shortly) this will not be the case.  With this in mind, all functions hace been written to operate on xmlNodePtr objects instead of xmlDocPtr objects.
-->
<!ELEMENT cib (nodes, resources, constraints, status)>

<!ATTLIST cib
          version      #CDATA       '1'
          generated    (true|false) 'true'
          timestamp    #CDATA       '0'>

<!-- list placholders -->
<!ELEMENT nodes       (node*)>
<!ATTLIST nodes>

<!ELEMENT resources   (resource*)>
<!ATTLIST resources>

<!ELEMENT constraints (constraint*)>
<!ATTLIST constraints)>

<!ELEMENT status      (state*)>
<!ATTLIST status>

<!-- the information carrying elements -->
<!ELEMENT node>
<!ATTLIST node
          id            #CDATA
          description   #CDATA
          node_type     (node|ping)                             'ping'
          health        (0|10|20|30|40|50|60|70|80|90|100)      '0'
          node_status   (down|up|active|stonith|stonith_failed) 'down'
          source        #CDATA
          timestamp     #CDATA  '0'>

<!-- the nvpairs in this case correspond to argument to the init scripts
<!ELEMENT resource (node_reference*,nvpair*)>
<!ATTLIST resource
          id            #CDATA
          description   #CDATA
          resource_cat  (none|ocf|init|resource|...)		  'none'
          resource_type (none|IPAddr|Nfs|Apache|Drbd|Stonith|...) 'none'
	  op_timeout	#CDATA
	  priority	#CDATA
          max_instances #CDATA  '1'
          timestamp     #CDATA  '0'>

<!ELEMENT node_reference>
<!-- 
action is only used during CIB updates.  To know if the node_reference should be added or removed from the resource
--> 
<!ATTLIST node_reference
          id            #CDATA
          action?       #CDATA
          weight        #CDATA
          timestamp     #CDATA  '0'>

<!ELEMENT constraint (nv_pair)*>
<!--
  r_id_2, var_name, var_value will only valid/required depending on the value of constraint_type
-->
<!ATTLIST constraint
          id                #CDATA
          description       #CDATA
          constraint_type   (None|StartAfter|SameNode|Block|...)     'none'
          r_id_1            #CDATA
          r_id_2?           #CDATA
          clear_on          (never|stonith|active|localRestart|...)  'never'
          timestamp         #CDATA      '0'>


<!ELEMENT nv_pair>
<!-- 
for consistency elsewhere, id is the name of the variable.
type may be used later on to distinguish between variable scopes.
action is only used during CIB updates.  To know if the nv_pair should be added or removed from the constraint
-->
<!ATTLIST nv_pair
          id                #CDATA
	  action?           #CDATA
          var_type          #CDATA
          var_value         #CDATA>

<!ELEMENT state>
<!-- runtime data -->
<!ATTLIST state
          id               #CDATA
          description      #CDATA
          res_id           #CDATA
          instance         #CDATA
          max_instances    #CDATA       '1'
          node_id          #CDATA
          resource_status  (stopped|starting|running|stopping|failed) 'stopped'
          source           #CDATA
          timestamp        #CDATA       '0'>

CIB: Sample:
======================

<cib version="1" generated="false" timestamp="1071655595">
        <nodes>
            <node id="node1" 
			name="production server" 
			node_type="node" 
			health="0" 
			node_status="down" 
			source="unknown" 
			timestamp="1071655595"/>
            <node id="node2" 
			name="an experimental 3GogoHz intel" 
			node_type="ping" 
			health="0" 
			node_status="down" 
			source="unknown" 
			timestamp="1071655595"/>
            <node id="node3" 
			name="an old sparc in the corner" 
			node_type="ping" 
			health="0" 
			node_status="down" 
			source="unknown" 
			timestamp="1071655595"/>
            <node id="node4" 
			name="my personal desktop" 
			node_type="node" 
			health="0" 
			node_status="down" 
			source="unknown" 
			timestamp="1071655595"/>
        </nodes>
        <resources>
            <resource id="res1" 
			resource_type="apache" 
			description="my web site" 
			op_timeout="20"
			priority="20"
			max_instance="1" 
			timestamp="1071655595">
                      <node_reference id="node2" 
					weight="20" 
					timestamp="1071655595"/>
            </resource>
            <resource id="res2" 
			resource_type="drbd" 
			description="apache data" 
			op_timeout="60"
			priority="20"
			max_instance="2" 
			timestamp="1071655595">
                      <node_reference id="node1" 
					weight="10" 
					timestamp="1071656274"/>
                      <node_reference id="node2" 
					weight="100" 
					timestamp="1071656274"/>
                      <node_reference id="node3" 
					weight="5" 
					timestamp="1071656274"/>
            </resource>
            <resource id="res3" 
			resource_type="dhcp" 
			description="dhcp" 
			op_timeout="45"
			priority="10"
			max_instance="1" 
			timestamp="1071655595">
                      <node_reference id="node2" 
					weight="-1" 
					timestamp="1071655595"/>
                      <node_reference id="node4" 
					weight="10" 
					timestamp="1071655595"/>
            </resource>
        </resources>
        <contraints>
            <contraint id="con1" 
			description="start apache after drbd" 
			constraint_type="StartAfter" 
			clear_on="never" 
			timestamp="1071655595" 
			res_id_1="res1" 
			res_id_2="res2"/>
            <contraint id="con2" 
			description="only start apache on systems where the kernel is 2.4.20-gentoo-r9" 
			constraint_type="SysVar" 
			clear_on="never" 
			timestamp="1071655595" 
			res_id_1="res1" 
			var_name="KERNEL_RELEASE" 
			var_value="2.4.20-gentoo-r9"/>
            <contraint id="failed-node1-res2-1" 
			description="system generated: instance 1 of res2 failed on node1" 
			constraint_type="Block" 
			clear_on="stonith" 
			timestamp="1071655595" 
			res_id_1="res2" 
			var_name="blockHost" 
			var_value="node1"/>
        </contraints>
        <status>
            <state id="res2-1" 
			res_id="res2" 
			instance="1" 
			max_instance="2" 
			node_id="node3" 
			resource_status="running" 
			source="none" 
			timestamp="1071656274"/>
            <state id="res3-1" 
			res_id="res3" 
			instance="1" 
			max_instance="1" 
			node_id="node4" 
			resource_status="running" 
			source="none" 
			timestamp="1071656274"/>
            <state id="res2-2" 
			res_id="res2" 
			instance="2" 
			max_instance="2" 
			node_id="node1" 
			resource_status="starting" 
			source="none" 
			timestamp="1071656274"/>
            <state id="res1-1" 
			res_id="res1" 
			instance="1" 
			max_instance="1" 
			node_id="node1" 
			resource_status="starting" 
			source="none" 
			timestamp="1071656274"/>
        </status>
</cib>
